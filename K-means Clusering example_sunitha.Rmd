---
title: "K-Means clustering"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
```{r}
install.packages("factoextra")
```

```{r}
library(tidyverse)
library(cluster)
library(factoextra)#factoring algorithms and visualization
```
To perform cluster analysis in R, data should be prepared as follows:
  1. rows are observations and columns are variables.
  2. Any missing value in the data must be removed or estimated.
  3. The data must be standardized(i.e. scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one.
  
  We are using built in data set USArrests which contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. It includes also the percent of the population living in urban areas

```{r}
df<-USArrests
view(USArrests)


```
REMOVE NULLS

to remove missing values
```{r}
df<-na.omit(df)
```
SCALE

as we dont want the clustering algorithm to depend to an arbitrary variable unit, we start by scaling/standardizing the data using R function scale:
```{r}
df<-scale(df)
head(df)
```
PICK CLUSTERING DISTANCE MEASURE

Clustering Distance Measures:
The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix. 
There are many methods to calculate this distance information. the choice of distance measure is a critical step in clustering. It defines how the similarity of two elements (x,y) is calculated and it will influence the shape of the clusters.
The classical methods for distance measures are
1.Euclidean a
2.Manhattan distances
other type of measures exists such as correlation -based distances. Correlation based distance is defined by subtracting the correlation coefficiant from 1. different methods used are
1. Pearson Correlation distance
2. Spearman correlation distance
  The spearman correlation method computes the correlation between the rank of x and the rank of y variables.
3. Kendall correlaiton distance
  Kendall correlation method measures the correspondence between the ranking of x and y variables. The total number of possible pairings of x with y observations is n(n − 1)/2, where n is the size of x and y. Begin by ordering the pairs by the x values. If x and y are correlated, then they would have the same relative rank orders. Now, for each 
yi, count the number of yj>yi (concordant pairs (c)) and the number of 
yj<yi (discordant pairs (d)).

The choice of distance measures is very important, as it has a strong influence on the clustering results. For most common clustering software, the default distance measure is the Euclidean distance. However, depending on the type of the data and the research questions, other dissimilarity measures might be preferred and you should be aware of the options

Within R it is simple to compute and visualize the distance matrix using the functions get_dist and fviz_dist from the factoextra R package. This starts to illustrate which states have large dissimilarities (red) versus those that appear to be fairly similar (teal)
get_dist: for computing a distance matrix between the rows of a data matrix. The default distance computed is the Euclidean; however, get_dist also supports distanced described in equations 2-5 above plus others.
fviz_dist: for visualizing a distance matrix

```{r}
distance <- get_dist(df)
fviz_dist(distance,gradient = list(low="#00AFBB",mid="white",high="#FC4E07"))

```
K-Means Clustering
K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given data set into a set of k groups (i.e. k clusters), where k represents the number of groups pre-specified by the analyst. It classifies objects in multiple groups (i.e., clusters), such that objects within the same cluster are as similar as possible (i.e., high intra-class similarity), whereas objects from different clusters are as dissimilar as possible (i.e., low inter-class similarity). In k-means clustering, each cluster is represented by its center (i.e, centroid) which corresponds to The Basic Idea
The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:the mean of points assigned to the cluster.

The total within-cluster sum of square measures the compactness (i.e goodness) of the clustering and we want it to be as small as possible

K-means summarizaiton:
1. define the number of clusters(K) to be created(by the analyst)
2. Select randomly k objects from the data set as initial cluster centers or means
3. Assigns each observation to their closest centroid, based on the eulidean distance between the object and the centroid.
4.For each of the k clusters update the cluster centroid by calculating the new  mean values of all the data points in the cluster.
5. Iteratively minimize the total within sum of square (Eq. 7). That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations.

We can compute k-means in R ith the kmeans function.Here we will group the data into two clusters(centers = 2). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart - 25 will generate 25 initial configurations. 

```{r}
k2<-kmeans(df,centers = 2,nstart=25)
str(k2)
```
The output of kmeans is a list with several bits of information. The most important being:

cluster: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.
centers: A matrix of cluster centers.
totss: The total sum of squares.
withinss: Vector of within-cluster sum of squares, one component per cluster.
tot.withinss: Total within-cluster sum of squares, i.e. sum(withinss).
betweenss: The between-cluster sum of squares, i.e. $totss-tot.withinss$.
size: The number of points in each cluster.

If we print the results we’ll see that our groupings resulted in 2 cluster sizes of 30 and 20. We see the cluster centers (means) for the two groups across the four variables (Murder, Assault, UrbanPop, Rape). We also get the cluster assignment for each observation (i.e. Alabama was assigned to cluster 2, Arkansas was assigned to cluster 1, etc.).

k2
```{r}
k2

```
We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.
```{r}
fviz_cluster(k2,data=df)
```
Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables.
```{r}
df %>% 
  as_tibble() %>% 
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>% 
  ggplot(aes(UrbanPop,Murder,color=factor(cluster),label =state))+
  geom_text()
```
Because the number of clusters (k) must be set before we start the algorithm, it is often advantageous to use several different values of k and examine the differences in the results. We can execute the same process for 3, 4, and 5 clusters, and the results are shown in the figure:
```{r}
k3<-kmeans(df,centers=3,nstart=25)
k4<-kmeans(df,centers=4,nstart=25)
k5<-kmeans(df,centers=5,nstart=25)
#plots to  compare
p1<-fviz_cluster(k2,geom="point",data=df)+ggtitle("k=2")
p2<-fviz_cluster(k3,geom="point",data=df)+ggtitle("k=3")
p3<-fviz_cluster(k4,geom="point",data=df)+ggtitle("k=4")
p4<-fviz_cluster(k5,geom="point",data=df)+ggtitle("k=5")

library(gridExtra)
grid.arrange(p1,p2,p3,p4,nrow =2)
```
Although this visual assessment tells us where true dilineations occur (or do not occur such as clusters 2 & 4 in the k = 5 graph) between clusters, it does not tell us what the optimal number of clusters is.

Determining Optimal Clusters:
As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters. To aid the analyst, the following explains the three most popular methods for determining the optimal clusters, which includes:

1. Elbow method
2. Silhouette method
3. Gap statistic

1. Elbow Method:
Recall that basic idea behind cluster partitioning methods, such as k-means clustering is to define clusters such that the total intra cluster variation(knows as total within cluster variation or total within-cluster sum of square) is minimized.
wss (with-in cluster sum or squares) measures the compactness of the clustering and we want it to be small as possible. 
Thus, we can use the following algorithm to define the optimal clusters.

1. Compute the clustering algorithm(k-means clustering) for different values of k. For instance, by varying k from (1 to 10) clusters.
2. For each k, calculate the total within-cluster sum of square(wss)
3. Plot the curve of wss according to the number of clusters k.
4. The location of the bend(Knee) in the plot is generally considered as an indicator of the appropriate number of clusters.

We can implement this in R with the following code. The results suggest that 4 is the optimal number of clusters as it appears to be the bend in the knee (or elbow).
```{r}
set.seed(123)

#function to calculate wss total within cluster sum of squares
wss<-function(k){
  kmeans(df,k,nstart=10)$tot.withinss
}

#compute wss for 2-15 clusters
k.values <- 1:15

#extract wss for 2-15 clusters
wss_values <- map_dbl(k.values,wss)

plot(k.values,wss_values,
     type = "b",pch=19,frame= FALSE,
     xlab="Number of cluster K",
     ylab="Total within-Clusters sum of squares")
```
Alternate method:
Fortunately, this process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):
```{r}
set.seed(123)
fviz_nbclust(df,kmeans,method="wss")
```

Average Silhouette Method
In short, the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k.2

We can use the silhouette function in the cluster package to compuate the average silhouette width. The following code computes this approach for 1-15 clusters. The results show that 2 clusters maximize the average silhouette values with 4 clusters coming in as second optimal number of clusters
```{r}
#function to compute average silhouette for k clusters
avg_sil<-function(k){
  km.res <-kmeans(df,centers=k,nstart=25)
  ss<-silhouette(km.res$cluster,dist(df))
  mean(ss[,3])
}
  #Compute and plot wss for k=2 to k = 15
  k.values<-2:15
  
  #extract avg silhouette for 2-15 clusters
  avg_sil_values <-map_dbl(k.values,avg_sil)
  
  plot(k.values,avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")
 

```
Similar to the elbow method, this process to compute the “average silhoutte method” has been wrapped up in a single function (fviz_nbclust):
```{r}
fviz_nbclust(df,kmeans,method="silhouette")
```
GaThe gap statistic has been published by R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001). The approach can be applied to any clustering method (i.e. K-means clustering, hierarchical clustering). The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering). The reference dataset is generated using Monte Carlo simulations of the sampling process. That is, for each variable (

 and generate values for the n points uniformly from the interval min to max.

For the observed data and the the reference data, the total intracluster variation is computed using different values of k. The gap statistic for a given k is defined as follow:
In short, the algorithm involves the following steps:

Cluster the observed data, varying the number of clusters from 

Generate B reference data sets and cluster each of them with varying number of clusters 
 
. Compute the estimated gap statistics presented in eq. 9.
Let 
¯
 compute the standard deviation 
 
 and define 
 
Choose the number of clusters as the smallest k such that 
G 
To compute the gap statistic method we can use the clusGap function which provides the gap statistic and standard error for an output.p statistic Method
```{r}
#Compute gap statistic method
set.seed(123)

gap_stat<- clusGap(df,FUN=kmeans, nstart=25, K.max = 10, B=50)

#Print the result
print(gap_stat,method = "firstmax")
```

We can visualize the results with fviz_gap_stat which suggests four clusters as the optimal number of clusters.

```{r}
fviz_gap_stat(gap_stat)
```
Extracting Results
With most of these approaches suggesting 4 as the number of optimal clusters, we can perform the final analysis and extract the results using 4 clusters.
```{r}
#Compute k-means clustering with k=4
set.seed(123)
final <-kmeans(df,4,nstart=25)
print(final)
```
```{r}
#Visualize the results using fviz_cluster
fviz_cluster(final,data=df)
```
And we can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level:
```{r}
USArrests %>% 
  mutate(Cluster = final$cluster) %>% 
  group_by(Cluster) %>% 
  summarise_all("mean")
```

Additional Comments
K-means clustering is a very simple and fast algorithm. Furthermore, it can efficiently deal with very large data sets. However, there are some weaknesses of the k-means approach.

One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters. Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of clusters. Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram. A future tutorial will illustrate the hierarchical clustering approach.

An additional disadvantage of K-means is that it’s sensitive to outliers and different results can occur if you change the ordering of your data. The Partitioning Around Medoids (PAM) clustering approach is less sensititive to outliers and provides a robust alternative to k-means to deal with these situations. A future tutorial will illustrate the PAM clustering approach.